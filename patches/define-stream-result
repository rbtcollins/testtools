Bottom: 672e6314751b3a560b122e4523f619d2804a36c9
Top:    dc41a04c58eeb326e0e1755d7a29171e413f120d
Author: Robert Collins <robertc@robertcollins.net>
Date:   2013-02-21 14:08:55 +1300

This defines a new sort of TestResult, StreamResult.

This is intended to prototype the new API for inclusion upstream.


---

diff --git a/NEWS b/NEWS
index b8ea878..23f8b9e 100644
--- a/NEWS
+++ b/NEWS
@@ -6,6 +6,22 @@ Changes and improvements to testtools_, grouped by release.
 NEXT
 ~~~~
 
+A new sort of TestResult, the StreamResult has been added, as a prototype for
+a revised standard library test result API. The API is not stable yet, though
+we will endeavour to preserve compatibility for early adopters.
+
+Improvements
+------------
+
+* New class ``StreamResult`` which defines the API for the new result type.
+  (Robert Collins)
+
+* ``PlaceHolder`` can now hold tags, and applies them before, and removes them
+  after, the test. (Robert Collins)
+
+* ``PlaceHolder`` can now hold timestamps, and applies them before the test and
+  then before the outcome. (Robert Collins)
+
 Improvements
 ------------
 
@@ -20,15 +36,6 @@ Improvements
   test that accidentally depended on the CPython repr for object().
   (Jonathan Lange)
 
-Improvements
-------------
-
-* ``PlaceHolder`` can now hold tags, and applies them before, and removes them
-  after, the test. (Robert Collins)
-
-* ``PlaceHolder`` can now hold timestamps, and applies them before the test and
-  then before the outcome. (Robert Collins)
-
 0.9.29
 ~~~~~~
 
diff --git a/doc/for-framework-folk.rst b/doc/for-framework-folk.rst
index 5e7d8dd..3bb4222 100644
--- a/doc/for-framework-folk.rst
+++ b/doc/for-framework-folk.rst
@@ -111,6 +111,60 @@ e.g.::
 Extensions to TestResult
 ========================
 
+StreamResult
+------------
+
+``StreamResult`` is a new API for dealing with test case progress that supports
+concurrent and distributed testing without the various issues that
+``TestResult`` has such as buffering in multiplexers.
+
+The design has several key principles:
+
+* Nothing that requires up-front knowledge of all tests.
+
+* Deal with tests running in concurrent environments, potentially distributed
+  across multiple processes (or even machines). This implies allowing multiple
+  tests to be active at once, supplying time explicitly, being able to
+  differentiate between tests running in different contexts and removing any
+  assumption that tests are necessarily in the same process.
+
+* Make the API as simple as possible - each aspect should do one thing well.
+
+The ``TestResult`` API this is intended to replace has three different clients.
+The testrunner running tests uses the API to find out whether the test run had
+errors, how many tests ran and so on. Secondly each executing ``TestCase``
+notifies the ``TestResult`` about activity. Finally, each ``TestCase`` queries
+the ``TestResult`` to see whether the test run should be aborted.
+
+With ``StreamResult`` we need to be able to provide a ``TestResult`` compatible
+adapter, to allow incremental migration. However, we don't need to conflate
+things long term. So - we define three separate APIs. Firstly there is the
+``StreamResult`` API which handles events generated by running tests. See the
+API documentation for ``testtools.StreamResult`` for details.
+
+StreamSummary
+-------------
+
+Secondly we define the ``StreamSummary`` API which takes responsibility for
+collating errors, detecting incomplete tests and counting tests. This provides
+a compatible API with those aspects of ``TestResult``. Again, see the API
+documentation for ``testtools.StreamSummary``.
+
+TestControl
+-----------
+
+Lastly we define the ``TestControl`` API which is used to provide the
+``shouldStop`` and ``stop`` elements from ``TestResult``. Again, see the API
+documentation for ``testtools.TestControl``.
+
+ThreadsafeStreamResult
+----------------------
+
+This is a ``StreamResult`` decorator for reporting tests from multiple threads
+at once. Each method takes out a lock around the decorated result to prevent
+race conditions. The ``startTestRun`` and ``stopTestRun`` methods are not
+forwarded to prevent the decorated result having them called multiple times.
+
 TestResult.addSkip
 ------------------
 
diff --git a/testtools/__init__.py b/testtools/__init__.py
index 707fb55..fa362af 100644
--- a/testtools/__init__.py
+++ b/testtools/__init__.py
@@ -25,6 +25,7 @@ __all__ = [
     'skip',
     'skipIf',
     'skipUnless',
+    'StreamResult',
     'ThreadsafeForwardingResult',
     'try_import',
     'try_imports',
@@ -66,6 +67,7 @@ else:
     from testtools.testresult import (
         ExtendedToOriginalDecorator,
         MultiTestResult,
+        StreamResult,
         Tagger,
         TestByTestResult,
         TestResult,
diff --git a/testtools/testresult/__init__.py b/testtools/testresult/__init__.py
index d37a772..bd1104f 100644
--- a/testtools/testresult/__init__.py
+++ b/testtools/testresult/__init__.py
@@ -5,6 +5,7 @@
 __all__ = [
     'ExtendedToOriginalDecorator',
     'MultiTestResult',
+    'StreamResult',
     'Tagger',
     'TestByTestResult',
     'TestResult',
@@ -16,6 +17,7 @@ __all__ = [
 from testtools.testresult.real import (
     ExtendedToOriginalDecorator,
     MultiTestResult,
+    StreamResult,
     Tagger,
     TestByTestResult,
     TestResult,
diff --git a/testtools/testresult/real.py b/testtools/testresult/real.py
index 7112ab6..8f24ecb 100644
--- a/testtools/testresult/real.py
+++ b/testtools/testresult/real.py
@@ -6,6 +6,7 @@ __metaclass__ = type
 __all__ = [
     'ExtendedToOriginalDecorator',
     'MultiTestResult',
+    'StreamResult',
     'Tagger',
     'TestResult',
     'TestResultDecorator',
@@ -244,6 +245,123 @@ class TestResult(unittest.TestResult):
         """
 
 
+class StreamResult(object):
+    """A test result for reporting the activity of a test run.
+
+    Typical use
+    -----------
+
+      >>> result = StreamResult()
+      >>> result.startTestRun()
+      >>> try:
+      ...     case.run(result)
+      ... finally:
+      ...     result.stopTestRun()
+
+    The case object will be either a TestCase or a TestSuite, and
+    generally make a sequence of calls like::
+
+      >>> result.status(self.id(), 'inprogress')
+      >>> result.status(self.id(), 'success')
+
+    General concepts
+    ----------------
+
+    StreamResult is built to process events that are emitted by tests during a
+    test run or test enumeration. The test run may be running concurrently, and
+    even be spread out across multiple machines.
+
+    All events are timestamped to prevent network buffering or scheduling
+    latency causing false timing reports. Timestamps are datetime objects in
+    the UTC timezone.
+
+    A route_code is a unicode string that identifies where a particular test
+    run. This is optional in the API but very useful when multiplexing multiple
+    streams together as it allows identification of interactions between tests
+    that were run on the same hardware or in the same test process. Generally
+    actual tests never need to bother with this - it is added and processed
+    by StreamResult's that do multiplexing / run analysis. route_code's are
+    also used to route stdin back to pdb instances.
+
+    The StreamResult base class does no accounting or processing, rather it
+    just provides an empty implementation of every method, suitable for use
+    as a base class regardless of intent.
+    """
+
+    def startTestRun(self):
+        """Start a test run.
+
+        This will prepare the test result to process results (which might imply
+        connecting to a database or remote machine).
+        """
+
+    def stopTestRun(self):
+        """Stop a test run.
+
+        This informs the result that no more test updates will be received. At
+        this point any test ids that have started and not completed can be
+        considered failed-or-hung.
+        """
+
+    def status(self, test_id=None, test_status=None, test_tags=None,
+        runnable=True, file_name=None, file_bytes=None, eof=False,
+        mime_type=None, route_code=None, timestamp=None):
+        """Inform the result about a test status.
+
+        :param test_id: The test whose status is being reported. None to 
+            report status about the test run as a whole.
+        :param test_status: The status for the test. There are two sorts of
+            status - interim and final status events. As many interim events
+            can be generated as desired, but only one final event. After a
+            final status event any further file or status events from the
+            same test_id+route_code may be discarded or associated with a new
+            test by the StreamResult. (But no exception will be thrown).
+            Interim states:
+            * None - no particular status is being reported, or status being
+              reported is not associated with a test (e.g. when reporting on
+              stdout / stderr chatter).
+            * inprogress - the test is currently running. Emitted by tests when
+              they start running and at any intermediary point they might
+              choose to indicate their continual operation.
+            Final states:
+            * exists - the test exists. This is used when a test is not being
+              executed. Typically this is when querying what tests could be run
+              in a test run (which is useful for selecting tests to run).
+            * xfail - the test failed but that was expected. This is purely
+              informative - the test is not considered to be a failure. 
+            * uxsuccess - the test passed but was expected to fail. The test
+              will be considered a failure.
+            * success - the test has finished without error.
+            * fail - the test failed (or errored). The test will be considered
+              a failure.
+            * skip - the test was selected to run but chose to be skipped. E.g.
+              a test dependency was missing. This is purely informative - the
+              test is not considered to be a failure.
+        :param test_tags: Optional set of tags to apply to the test. Tags
+            have no intrinsic meaning - that is up to the test author.
+        :param runnable: Allows status reports to mark that they are for
+            tests which are not able to be explicitly run. For instance,
+            subtests will report themselves as non-runnable.
+        :param file_name: The name for the file_bytes. Any unicode string may
+            be used. While there is no semantic value attached to the name
+            of any attachment, the names 'stdout' and 'stderr' and 'traceback'
+            are recommended for use only for output sent to stdout, stderr and
+            tracebacks of exceptions. When file_name is supplied, file_bytes
+            must be a bytes instance.
+        :param file_bytes: A bytes object containing content for the named
+            file. This can just be a single chunk of the file - emitting
+            another file event with more later. Must be None unleses a
+            file_name is supplied.
+        :param eof: This chunk is the last chunk of the file, any additional
+            chunks with the same name should be treated as an error and 
+            discarded. Ignored unless file_name has been supplied.
+        :param mime_type: An optional MIME type for the file. stdout and
+            stderr will generally be "text/plain; charset=utf8". If None,
+            defaults to application/octet-stream. Ignores unless file_name
+            has been supplied.
+        """
+
+
 class MultiTestResult(TestResult):
     """A test result that dispatches to many test results."""
 
diff --git a/testtools/tests/test_testresult.py b/testtools/tests/test_testresult.py
index 64bb743..397f058 100644
--- a/testtools/tests/test_testresult.py
+++ b/testtools/tests/test_testresult.py
@@ -21,6 +21,7 @@ from testtools import (
     ExtendedToOriginalDecorator,
     MultiTestResult,
     PlaceHolder,
+    StreamResult,
     Tagger,
     TestCase,
     TestResult,
@@ -446,6 +447,75 @@ class TestTestResultDecoratorContract(TestCase, StartTestRunContract):
         return TestResultDecorator(TestResult())
 
 
+class TestStreamResultContract(object):
+
+    def _make_result(self):
+        raise NotImplementedError(self._make_result)
+
+    def test_startTestRun(self):
+        result = self._make_result()
+        result.startTestRun()
+        result.stopTestRun()
+
+    def test_files(self):
+        result = self._make_result()
+        result.startTestRun()
+        self.addCleanup(result.stopTestRun)
+        now = datetime.datetime.now(utc)
+        inputs = list(dict(
+            eof=True,
+            mime_type="text/plain",
+            route_code=_u("1234"),
+            test_id=_u("foo"),
+            timestamp=now,
+            ).items())
+        param_dicts = self._permute(inputs)
+        for kwargs in param_dicts:
+            result.status(file_name=_u("foo"), file_bytes=_b(""), **kwargs)
+            result.status(file_name=_u("foo"), file_bytes=_b("bar"), **kwargs)
+
+    def test_test_status(self):
+        result = self._make_result()
+        result.startTestRun()
+        self.addCleanup(result.stopTestRun)
+        now = datetime.datetime.now(utc)
+        args = [[_u("foo"), s] for s in ['exists', 'inprogress', 'xfail',
+            'uxsuccess', 'success', 'fail', 'skip']]
+        inputs = list(dict(
+            runnable=False,
+            test_tags=set(['quux']),
+            route_code=_u("1234"),
+            timestamp=now,
+            ).items())
+        param_dicts = self._permute(inputs)
+        for kwargs in param_dicts:
+            for arg in args:
+                result.status(test_id=arg[0], test_status=arg[1], **kwargs)
+
+    def _permute(self, inputs):
+        param_dicts = [{}]
+        # Build a full set of combinations
+        def permutations(size, inputs):
+            # For each possible start point, return the permutations one size
+            # smaller from the rest of the list combined with that start point.
+            if not size:
+                return [[]]
+            result = []
+            for start in range(len(inputs)-size+1):
+                for permutation in permutations(size-1, inputs[start+1:]):
+                    result.append([inputs[start]] + permutation)
+            return result
+        for size in range(1, len(inputs)+1):
+            param_dicts.extend(dict(p) for p in permutations(size, inputs))
+        return param_dicts
+
+
+class TestBaseStreamResultContract(TestCase, TestStreamResultContract):
+
+    def _make_result(self):
+        return StreamResult()
+
+
 class TestTestResult(TestCase):
     """Tests for 'TestResult'."""
