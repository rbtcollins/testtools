Bottom: 46f9217888c9acebd399b8bae5da7220e1c019e9
Top:    1b99ea48ced884eb0b687d567a6340f4577adb57
Author: Robert Collins <robertc@robertcollins.net>
Date:   2013-02-16 20:12:25 +1300

Add StreamSummary to generate summary statistics and capture failures.

This is equivalent the to behaviour in the TestResult class, but split into a
dedicated helper (so that decorators and the like don't need to waste cycles
generating them).


---

diff --git a/setup.py b/setup.py
index 2e079bf..2ad6a62 100755
--- a/setup.py
+++ b/setup.py
@@ -69,5 +69,7 @@ setup(name='testtools',
       zip_safe=False,
       install_requires=[
         'extras',
+        # 'mimetype' has not been uploaded by the maintainer with Python3 compat
+        'python-mimetype',
         ],
       )
diff --git a/testtools/__init__.py b/testtools/__init__.py
index 9e64be2..2ea5542 100644
--- a/testtools/__init__.py
+++ b/testtools/__init__.py
@@ -27,6 +27,7 @@ __all__ = [
     'skipIf',
     'skipUnless',
     'StreamResult',
+    'StreamSummary',
     'ThreadsafeForwardingResult',
     'try_import',
     'try_imports',
@@ -70,6 +71,7 @@ else:
         ExtendedToOriginalDecorator,
         MultiTestResult,
         StreamResult,
+        StreamSummary,
         Tagger,
         TestByTestResult,
         TestResult,
diff --git a/testtools/testresult/__init__.py b/testtools/testresult/__init__.py
index f1ec047..9aa6b7e 100644
--- a/testtools/testresult/__init__.py
+++ b/testtools/testresult/__init__.py
@@ -7,6 +7,7 @@ __all__ = [
     'ExtendedToOriginalDecorator',
     'MultiTestResult',
     'StreamResult',
+    'StreamSummary',
     'Tagger',
     'TestByTestResult',
     'TestResult',
@@ -20,6 +21,7 @@ from testtools.testresult.real import (
     ExtendedToOriginalDecorator,
     MultiTestResult,
     StreamResult,
+    StreamSummary,
     Tagger,
     TestByTestResult,
     TestResult,
diff --git a/testtools/testresult/real.py b/testtools/testresult/real.py
index 1d4aff9..787e3a4 100644
--- a/testtools/testresult/real.py
+++ b/testtools/testresult/real.py
@@ -19,13 +19,19 @@ import sys
 import unittest
 
 from extras import safe_hasattr
+from mimeparse import parse_mime_type
 
 from testtools.compat import all, str_is_unicode, _u
 from testtools.content import (
+    Content,
     text_content,
     TracebackContent,
     )
+from testtools.content_type import ContentType
 from testtools.tags import TagContext
+# circular import
+# from testtools.testcase import PlaceHolder
+PlaceHolder = None
 
 # From http://docs.python.org/library/datetime.html
 _ZERO = datetime.timedelta(0)
@@ -403,6 +409,113 @@ class CopyStreamResult(object):
         domap(methodcaller('status', *args, **kwargs), self.targets)
 
 
+class StreamSummary(StreamResult):
+    """A specialised StreamResult that summarises a stream.
+    
+    The summary uses the same representation as the original
+    unittest.TestResult contract, allowing it to be consumed by any test
+    runner.
+    """
+
+    def startTestRun(self):
+        super(StreamSummary, self).startTestRun()
+        self.failures = []
+        self.errors = []
+        self.testsRun = 0
+        self.skipped = []
+        self.expectedFailures = []
+        self.unexpectedSuccesses = []
+        # Maps (id, route_code) -> a PlaceHolder
+        global PlaceHolder
+        from testtools.testcase import PlaceHolder
+        self._inprogress = {}
+        self._handle_final_status = {
+            'success': self._success,
+            'skip': self._skip,
+            'exists': self._exists,
+            'fail': self._fail,
+            'xfail': self._xfail,
+            'uxsuccess': self._uxsuccess,
+            }
+
+    def stopTestRun(self):
+        super(StreamSummary, self).stopTestRun()
+        self.testsRun += len(self._inprogress)
+        for case in self._inprogress.values():
+            self.errors.append((case, "Test did not complete"))
+        self._inprogress.clear()
+
+    def file(self, file_name, file_bytes, eof=False, mime_type=None,
+        test_id=None, route_code=None, timestamp=None):
+        super(StreamSummary, self).file(file_name, file_bytes, eof=eof,
+            mime_type=mime_type, test_id=test_id, route_code=route_code,
+            timestamp=timestamp)
+        key = self._ensure_key(test_id, route_code)
+        if key:
+            case = self._inprogress[key]
+            if file_name not in case._details:
+                if mime_type is None:
+                    mime_type = 'application/octet-stream'
+                primary, sub, parameters = parse_mime_type(mime_type)
+                content_type = ContentType(primary, sub, parameters)
+                content_bytes = []
+                case._details[file_name] = Content(
+                    content_type, lambda:content_bytes)
+            case._details[file_name].iter_bytes().append(file_bytes)
+
+    def status(self, test_id, test_status, test_tags=None, runnable=True,
+        route_code=None, timestamp=None):
+        super(StreamSummary, self).status(test_id, test_status,
+            test_tags=test_tags, runnable=runnable, route_code=route_code,
+            timestamp=timestamp)
+        key = self._ensure_key(test_id, route_code)
+        if test_status != 'inprogress':
+            case = self._inprogress.pop(key)
+            self._handle_final_status[test_status](
+                case, test_tags, runnable, route_code, timestamp)
+    
+    def _ensure_key(self, test_id, route_code):
+        if test_id is None:
+            return
+        key = (test_id, route_code)
+        if key not in self._inprogress:
+            self._inprogress[key] = PlaceHolder(test_id, outcome='unknown')
+        return key
+
+    def _success(self, case, test_tags, runnable, route_code, timestamp):
+        pass
+
+    def _skip(self, case, test_tags, runnable, route_code, timestamp):
+        case._outcome = 'addSkip'
+        if 'reason' not in case._details:
+            reason = "Unknown"
+        else:
+            reason = case._details['reason'].as_text()
+        self.skipped.append((case, reason))
+
+    def _exists(self, case, test_tags, runnable, route_code, timestamp):
+        pass
+
+    def _fail(self, case, test_tags, runnable, route_code, timestamp):
+        pass
+
+    def _xfail(self, case, test_tags, runnable, route_code, timestamp):
+        pass
+
+    def _uxsuccess(self, case, test_tags, runnable, route_code, timestamp):
+        pass
+
+    def wasSuccessful(self):
+        """Return False if any failure has occured.
+
+        Note that incomplete tests can only be detected when stopTestRun is
+        called, so that should be called before checking wasSuccessful.
+        """
+        return (not self.failures and
+            not self.errors and
+            not self.expectedFailures and not self.unexpectedSuccesses)
+
+
 class MultiTestResult(TestResult):
     """A test result that dispatches to many test results."""
 
diff --git a/testtools/tests/test_testresult.py b/testtools/tests/test_testresult.py
index 2abfdff..8e4fc5e 100644
--- a/testtools/tests/test_testresult.py
+++ b/testtools/tests/test_testresult.py
@@ -23,6 +23,7 @@ from testtools import (
     MultiTestResult,
     PlaceHolder,
     StreamResult,
+    StreamSummary,
     Tagger,
     TestCase,
     TestResult,
@@ -53,6 +54,7 @@ from testtools.matchers import (
     Contains,
     DocTestMatches,
     Equals,
+    HasLength,
     MatchesAny,
     MatchesException,
     Raises,
@@ -542,6 +544,12 @@ class TestDoubleStreamResultContract(TestCase, TestStreamResultContract):
         return LoggingStreamResult()
 
 
+class TestStreamSummaryResultContract(TestCase, TestStreamResultContract):
+
+    def _make_result(self):
+        return StreamSummary()
+
+
 class TestDoubleStreamResultEvents(TestCase):
 
     def test_startTestRun(self):
@@ -632,6 +640,97 @@ class TestCopyStreamResultCopies(TestCase):
                 ])))
 
 
+class TestStreamSummary(TestCase):
+
+    def test_attributes(self):
+        result = StreamSummary()
+        result.startTestRun()
+        self.assertEqual([], result.failures)
+        self.assertEqual([], result.errors)
+        self.assertEqual([], result.skipped)
+        self.assertEqual([], result.expectedFailures)
+        self.assertEqual([], result.unexpectedSuccesses)
+        self.assertEqual(0, result.testsRun)
+
+    def test_startTestRun(self):
+        result = StreamSummary()
+        result.startTestRun()
+        result.failures.append('x')
+        result.errors.append('x')
+        result.skipped.append('x')
+        result.expectedFailures.append('x')
+        result.unexpectedSuccesses.append('x')
+        result.testsRun = 1
+        result.startTestRun()
+        self.assertEqual([], result.failures)
+        self.assertEqual([], result.errors)
+        self.assertEqual([], result.skipped)
+        self.assertEqual([], result.expectedFailures)
+        self.assertEqual([], result.unexpectedSuccesses)
+        self.assertEqual(0, result.testsRun)
+
+    def test_wasSuccessful(self):
+        # wasSuccessful returns False if any of
+        # failures/errors/expectedFailures/unexpectedSuccesses is
+        # non-empty.
+        result = StreamSummary()
+        result.startTestRun()
+        self.assertEqual(True, result.wasSuccessful())
+        result.failures.append('x')
+        self.assertEqual(False, result.wasSuccessful())
+        result.startTestRun()
+        result.errors.append('x')
+        self.assertEqual(False, result.wasSuccessful())
+        result.startTestRun()
+        result.skipped.append('x')
+        self.assertEqual(True, result.wasSuccessful())
+        result.startTestRun()
+        result.expectedFailures.append('x')
+        self.assertEqual(False, result.wasSuccessful())
+        result.startTestRun()
+        result.unexpectedSuccesses.append('x')
+        self.assertEqual(False, result.wasSuccessful())
+
+    def test_stopTestRun(self):
+        result = StreamSummary()
+        # terminal successful codes.
+        result.startTestRun()
+        result.status("foo", "inprogress")
+        result.status("foo", "success")
+        result.status("bar", "skip")
+        result.status("baz", "exists")
+        result.stopTestRun()
+        self.assertEqual(True, result.wasSuccessful())
+        # Tests inprogress at stopTestRun trigger a failure.
+        result.startTestRun()
+        result.status("foo", "inprogress")
+        result.stopTestRun()
+        self.assertEqual(False, result.wasSuccessful())
+        self.assertThat(result.errors, HasLength(1))
+        self.assertEqual("foo", result.errors[0][0].id())
+        self.assertEqual("Test did not complete", result.errors[0][1])
+        # interim state detection handles route codes - while duplicate ids in
+        # one run is undesirable, it may happen (e.g. with repeated tests).
+        result.startTestRun()
+        result.status("foo", "inprogress")
+        result.status("foo", "inprogress", route_code="A")
+        result.status("foo", "success", route_code="A")
+        result.stopTestRun()
+        self.assertEqual(False, result.wasSuccessful())
+
+    def test_status_skip(self):
+        # when skip is seen, a synthetic test is reported with reason captured
+        # from the 'reason' file attachment if any.
+        result = StreamSummary()
+        result.startTestRun()
+        result.file("reason", _b("Missing dependency"), eof=True,
+            mime_type="text/plain; charset=utf8", test_id="foo.bar")
+        result.status("foo.bar", "skip")
+        self.assertThat(result.skipped, HasLength(1))
+        self.assertEqual("foo.bar", result.skipped[0][0].id())
+        self.assertEqual(_u("Missing dependency"), result.skipped[0][1])
+
+
 class TestTestResult(TestCase):
     """Tests for 'TestResult'."""
